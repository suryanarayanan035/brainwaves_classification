{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ab2a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-11 07:21:20.887125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-11 07:21:20.887145: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee980e19",
   "metadata": {},
   "source": [
    "## Before running cells below,be sure to remove the below files\n",
    "\n",
    "### These files contain less samples or no samples at all. Therefore these files are not usable.\n",
    "\n",
    "Bad Files\n",
    " - OpenBCI-RAW-2021-08-19_15-09-22.txt(Right)\n",
    " - OpenBCI-RAW-2021-08-19_15-02-47.txt(Right)\n",
    " - OpenBCI-RAW-2021-08-19_15-03-12.txt(Right)\n",
    " - OpenBCI-RAW-2021-08-19_15-03-03.txt(Right)\n",
    " - OpenBCI-RAW-2021-08-19_15-00-14.txt(Left)\n",
    " - OpenBCI-RAW-2021-08-19_15-00-07.txt(Left)\n",
    " - OpenBCI-RAW-2021-08-19_15-00-25.txt(Left)\n",
    " - OpenBCI-RAW-2021-08-19_15-00-42.txt(Left)\n",
    " - OpenBCI-RAW-2021-08-19_14-59-40.txt(left)\n",
    " - OpenBCI-RAW-2021-08-19_15-00-54.txt(left)\n",
    " \n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e05393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pandas_dataset():\n",
    "    dataset_root_folder = \"thoughts\"\n",
    "    dataset = pd.DataFrame()\n",
    "    class_folders = os.listdir(dataset_root_folder)\n",
    "    timeframe_start = 251\n",
    "    timeframe_end = 1501\n",
    "    for class_folder in class_folders:\n",
    "        if class_folder == \".ipynb_checkpoints\" or class_folder == \"pick_object\":\n",
    "            continue\n",
    "        for file in os.listdir(dataset_root_folder+\"/\"+class_folder):\n",
    "            file_name = dataset_root_folder+\"/\"+class_folder+\"/\"+file \n",
    "            if file == \".ipynb_checkpoints\":\n",
    "                continue\n",
    "            if dataset.empty:\n",
    "                dataset = pd.read_csv(file_name,skiprows=6,header=None)\n",
    "                dataset=dataset.iloc[timeframe_start:timeframe_end,1:9]\n",
    "                dataset[\"class\"]=class_folder\n",
    "            else:\n",
    "                tmp_dataset=pd.read_csv(file_name,skiprows=6,header=None)\n",
    "                tmp_dataset=tmp_dataset.iloc[timeframe_start:timeframe_end,1:9]\n",
    "                tmp_dataset[\"class\"]=1 if class_folder == \"left\" else 2 \n",
    "                dataset=dataset.append(tmp_dataset,ignore_index=True)\n",
    "            \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7daf7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a tuple containing signals and labels (signals,labels)\n",
    "\n",
    "def prepare_tuple_dataset():\n",
    "    dataset_root_folder = \"thoughts\"\n",
    "    signals = []\n",
    "    labels = []\n",
    "    class_folders = os.listdir(dataset_root_folder) #folders of seperate signals like left,right and pick_object\n",
    "    #timeframe_starrt and timeframe_end selects from which sample to which sample\n",
    "    # should be used for training and evaluation \n",
    "    timeframe_start = 251 #from which sample to st\n",
    "    timeframe_end = 1501\n",
    "    for class_folder in class_folders:\n",
    "        if class_folder == \".ipynb_checkpoints\" or class_folder == \"pick_object\":\n",
    "            continue\n",
    "        for file in os.listdir(dataset_root_folder+\"/\"+class_folder):\n",
    "            file_name = dataset_root_folder+\"/\"+class_folder+\"/\"+file \n",
    "            if file == \".ipynb_checkpoints\":\n",
    "                continue\n",
    "            else:\n",
    "                tmp_dataset = pd.read_csv(file_name,skiprows=6,header=None)\n",
    "                tmp_dataset=tmp_dataset.iloc[timeframe_start:timeframe_end,1:9]\n",
    "                signals.append(tmp_dataset.T.to_numpy().reshape(8,1250))\n",
    "                labels.append(np.array(1 if class_folder == \"left\" else 0).reshape(1,1))\n",
    "            \n",
    "    return signals,labels\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def scale_signals(signals):\n",
    "    for i in range(len(signals)):\n",
    "        for j in range(len(signals[i])):\n",
    "            scaler = MinMaxScaler()\n",
    "            signals[i] = [scaler.fit_transform(channel.reshape(1,-1)).ravel() for channel in signals[i]]\n",
    "            \n",
    "    return signals\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00ce04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import stft\n",
    "import librosa\n",
    "def calculate_stft(signals,frame_size=250,hop_length=125,sampling_frequency=250.0):\n",
    "    stfts = []\n",
    "    for signal in signals:\n",
    "        channel_stfts=[]\n",
    "        for channel in signal:\n",
    "            channel_stft=librosa.stft(channel,\n",
    "                                    n_fft=frame_size,\n",
    "                                    hop_length=hop_length\n",
    "                                    )\n",
    "            channel_stfts.append(np.abs(channel_stft)**2)\n",
    "        stfts.append(np.array(channel_stfts))\n",
    "    return stfts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8915206f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 896 into shape (8,1250)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24122/3741985408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msignals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_tuple_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msignals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_stft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_24122/3670027975.py\u001b[0m in \u001b[0;36mprepare_tuple_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtmp_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskiprows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mtmp_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtmp_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtimeframe_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtimeframe_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0msignals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclass_folder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"left\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 896 into shape (8,1250)"
     ]
    }
   ],
   "source": [
    "signals,labels = prepare_tuple_dataset()\n",
    "signals = calculate_stft(signals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b155b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(signals)):\n",
    "    signals[i]=signals[i].reshape((126,11,8))\n",
    "    print(\"shape\",signals[i].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784747a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM,Flatten,Dense,InputLayer,Reshape,Bidirectional,Conv2D,MaxPool2D,BatchNormalization\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(128,(2,2),padding=\"same\",input_shape=(126,11,8),activation=\"leaky_relu\",kernel_initializer=\"he_normal\"),\n",
    "        MaxPool2D((2,2)),\n",
    "        Conv2D(256,(2,2),activation=\"leaky_relu\",kernel_initializer=\"he_normal\",padding=\"same\"),\n",
    "        Conv2D(512,(2,2),activation=\"leaky_relu\",kernel_initializer=\"he_normal\",padding=\"same\"),        \n",
    "        MaxPool2D((2,2)),\n",
    "        Flatten(),\n",
    "        Dense(1,activation=\"softmax\",name=\"output_layer\"),\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=Adam(0.000001),metrics=[\"accuracy\"])\n",
    "    return model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "history = model.fit(tf_dataset,epochs=epochs,validation_data=tf_dataset)\n",
    "\n",
    "loss_train = history.history['loss']\n",
    "# loss_val = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(1,epochs+1)\n",
    "plt.plot(epochs_range, loss_train, 'g', label='Training loss')\n",
    "# plt.plot(epochs_range, loss_val, 'b', label='validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(tf_dataset)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_spectrogram(signal,sampling_rate,hop_length,y_axis=\"linear\"):\n",
    "    \n",
    "  plt.figure(figsize=(25,10))\n",
    "  librosa.display.specshow(signal,\n",
    "                            sr=sampling_rate,\n",
    "                            hop_length=hop_length,\n",
    "                            x_axis=\"time\",\n",
    "                            y_axis=y_axis)\n",
    "  plt.colorbar(format=\"%+2.f\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de500bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM,Flatten,Dense,InputLayer,Reshape,Bidirectional\n",
    "from tensorflow.keras import Sequential\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        InputLayer((8,1250)),\n",
    "        Reshape((125,80)),\n",
    "        LSTM(units=64,name=\"LSTM_1\"),\n",
    "        Dense(32,activation=\"relu\"),\n",
    "        Dense(1,activation=\"sigmoid\",name=\"output_layer\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    return model\n",
    "model = build_model()\n",
    "model.summary()\n",
    "model.fit(tf_dataset,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e882b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM,Flatten,Dense,InputLayer,Reshape,Bidirectional\n",
    "from tensorflow.keras import Sequential\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        InputLayer((8,1250)),\n",
    "        Reshape((125,80)),\n",
    "        LSTM(units=64,name=\"LSTM_1\"),\n",
    "        Reshape((64,1)),\n",
    "        LSTM(units=128,name=\"LSTM_2\"),\n",
    "        Dense(32,activation=\"relu\"),\n",
    "        Dense(1,activation=\"sigmoid\",name=\"output_layer\")\n",
    "    ])\n",
    "    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ac095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_dataset,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed5876",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84148c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c4b1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
